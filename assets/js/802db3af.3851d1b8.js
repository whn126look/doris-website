"use strict";(self.webpackChunkdoris_website=self.webpackChunkdoris_website||[]).push([["734352"],{171961:function(e,n,r){r.r(n),r.d(n,{metadata:()=>s,contentTitle:()=>a,default:()=>u,assets:()=>l,toc:()=>c,frontMatter:()=>i});var s=JSON.parse('{"id":"admin-manual/workload-management/best-practice/workload-group-best-practice","title":"Workload Group Best Practice","description":"\x3c!--","source":"@site/versioned_docs/version-3.0/admin-manual/workload-management/best-practice/workload-group-best-practice.md","sourceDirName":"admin-manual/workload-management/best-practice","slug":"/admin-manual/workload-management/best-practice/workload-group-best-practice","permalink":"/docs/3.0/admin-manual/workload-management/best-practice/workload-group-best-practice","draft":false,"unlisted":false,"tags":[],"version":"3.0","frontMatter":{"title":"Workload Group Best Practice","language":"en"},"sidebar":"docs","previous":{"title":"Grouping Workload Groups","permalink":"/docs/3.0/admin-manual/workload-management/best-practice/group-workload-groups"},"next":{"title":"SQL Interception","permalink":"/docs/3.0/admin-manual/query-admin/sql-interception"}}'),o=r("785893"),t=r("250065");let i={title:"Workload Group Best Practice",language:"en"},a=void 0,l={},c=[{value:"Test hard memory limit",id:"test-hard-memory-limit",level:2},{value:"Test env",id:"test-env",level:3},{value:"Not using Workload Group&#39;s memory limit.",id:"not-using-workload-groups-memory-limit",level:3},{value:"Use Workload Group limit memory",id:"use-workload-group-limit-memory",level:3},{value:"Suggestions",id:"suggestions",level:3},{value:"CPU hard limit Test",id:"cpu-hard-limit-test",level:2},{value:"Test env",id:"test-env-1",level:3},{value:"Test",id:"test",level:3},{value:"NOTE",id:"note",level:3},{value:"Test limit local IO",id:"test-limit-local-io",level:2},{value:"Test",id:"test-1",level:3},{value:"Not limit IO",id:"not-limit-io",level:3},{value:"Test IO limit.",id:"test-io-limit",level:3},{value:"NOTE",id:"note-1",level:3},{value:"Test limit remote IO",id:"test-limit-remote-io",level:2},{value:"Test env",id:"test-env-2",level:3},{value:"Test not limit remote IO",id:"test-not-limit-remote-io",level:3},{value:"Test limit remote IO",id:"test-limit-remote-io-1",level:3}];function d(e){let n={code:"code",h2:"h2",h3:"h3",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",...(0,t.a)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.h2,{id:"test-hard-memory-limit",children:"Test hard memory limit"}),"\n",(0,o.jsx)(n.p,{children:"Ad-hoc queries typically have unpredictable SQL input and uncertain memory usage, which carries the risk of a few queries consuming a large amount of memory. This type of load can be assigned to a separate group. By using the hard memory limits feature of the Workload Group, sudden large queries can be prevented from consuming all available memory, which would otherwise leave no memory for other queries or cause an Out of Memory (OOM) error."}),"\n",(0,o.jsx)(n.p,{children:"When the memory usage of this Workload Group exceeds the configured hard limit, memory will be freed by killing queries, thus preventing the process memory from being completely consumed."}),"\n",(0,o.jsx)(n.h3,{id:"test-env",children:"Test env"}),"\n",(0,o.jsx)(n.p,{children:"1FE\uFF0C1BE\uFF0CBE(96 cores)\uFF0Cmemory is 375G\u3002"}),"\n",(0,o.jsx)(n.p,{children:"Test data is clickbench, run q29 in 3 concurrent."}),"\n",(0,o.jsx)(n.h3,{id:"not-using-workload-groups-memory-limit",children:"Not using Workload Group's memory limit."}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"Show process memory usage, ps shows memory usage, the memory is 7.7G."}),"\n"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:"[ ~]$ ps -eo pid,comm,%mem,rss | grep 1407481\n1407481 doris_be         2.0 7896792\n[ ~]$ ps -eo pid,comm,%mem,rss | grep 1407481\n1407481 doris_be         2.0 7929692\n[ ~]$ ps -eo pid,comm,%mem,rss | grep 1407481\n1407481 doris_be         2.0 8101232\n"})}),"\n",(0,o.jsxs)(n.ol,{start:"2",children:["\n",(0,o.jsx)(n.li,{children:"Show Workload Group memory by system table, it's 5.8G."}),"\n"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:"mysql [information_schema]>select MEMORY_USAGE_BYTES / 1024/ 1024 as wg_mem_used_mb from workload_group_resource_usage where workload_group_id=11201;\n+-------------------+\n| wg_mem_used_mb    |\n+-------------------+\n| 5797.524360656738 |\n+-------------------+\n1 row in set (0.01 sec)\n\nmysql [information_schema]>select MEMORY_USAGE_BYTES / 1024/ 1024 as wg_mem_used_mb from workload_group_resource_usage where workload_group_id=11201;\n+-------------------+\n| wg_mem_used_mb    |\n+-------------------+\n| 5840.246627807617 |\n+-------------------+\n1 row in set (0.02 sec)\n\nmysql [information_schema]>select MEMORY_USAGE_BYTES / 1024/ 1024 as wg_mem_used_mb from workload_group_resource_usage where workload_group_id=11201;\n+-------------------+\n| wg_mem_used_mb    |\n+-------------------+\n| 5878.394917488098 |\n+-------------------+\n1 row in set (0.02 sec)\n"})}),"\n",(0,o.jsx)(n.p,{children:"Here, you can see that the memory usage of the process is usually much greater than the memory usage of a single Workload Group, even if only one Workload Group is running in the process. This is because the Workload Group only accounts for the memory used by queries and some parts of data import. Other components within the process, such as metadata and various caches, are not included in the Workload Group's memory usage and are not managed by the Workload Group."}),"\n",(0,o.jsx)(n.h3,{id:"use-workload-group-limit-memory",children:"Use Workload Group limit memory"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"Alter workload group."}),"\n"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:"alter workload group g2 properties('memory_limit'='0.5%');\nalter workload group g2 properties('enable_memory_overcommit'='false');\n"})}),"\n",(0,o.jsxs)(n.ol,{start:"2",children:["\n",(0,o.jsx)(n.li,{children:"Run test, the workload group uses 1.5G memory."}),"\n"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:"mysql [information_schema]>select MEMORY_USAGE_BYTES / 1024/ 1024 as wg_mem_used_mb from workload_group_resource_usage where workload_group_id=11201;\n+--------------------+\n| wg_mem_used_mb     |\n+--------------------+\n| 1575.3877239227295 |\n+--------------------+\n1 row in set (0.02 sec)\n\nmysql [information_schema]>select MEMORY_USAGE_BYTES / 1024/ 1024 as wg_mem_used_mb from workload_group_resource_usage where workload_group_id=11201;\n+------------------+\n| wg_mem_used_mb   |\n+------------------+\n| 1668.77405834198 |\n+------------------+\n1 row in set (0.01 sec)\n\nmysql [information_schema]>select MEMORY_USAGE_BYTES / 1024/ 1024 as wg_mem_used_mb from workload_group_resource_usage where workload_group_id=11201;\n+--------------------+\n| wg_mem_used_mb     |\n+--------------------+\n| 499.96760272979736 |\n+--------------------+\n1 row in set (0.01 sec)\n"})}),"\n",(0,o.jsxs)(n.ol,{start:"3",children:["\n",(0,o.jsx)(n.li,{children:"Show memory by ps command, the max memory is 3.8G."}),"\n"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:"[ ~]$ ps -eo pid,comm,%mem,rss | grep 1407481\n1407481 doris_be         1.0 4071364\n[ ~]$ ps -eo pid,comm,%mem,rss | grep 1407481\n1407481 doris_be         1.0 4059012\n[ ~]$ ps -eo pid,comm,%mem,rss | grep 1407481\n1407481 doris_be         1.0 4057068\n"})}),"\n",(0,o.jsxs)(n.ol,{start:"4",children:["\n",(0,o.jsx)(n.li,{children:"There are many query failed because of oom."}),"\n"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:'1724074250162,14126,1c_sql,HY000 1105,"java.sql.SQLException: errCode = 2, detailMessage = (127.0.0.1)[MEM_LIMIT_EXCEEDED]GC wg for hard limit, wg id:11201, name:g2, used:1.71 GB, limit:1.69 GB, backend:10.16.10.8. cancel top memory used tracker <Query#Id=4a0689936c444ac8-a0d01a50b944f6e7> consumption 1.71 GB. details:process memory used 3.01 GB exceed soft limit 304.41 GB or sys available memory 101.16 GB less than warning water mark 12.80 GB., Execute again after enough memory, details see be.INFO.",\u5E76\u53D1 1-3,text,false,,444,0,3,3,null,0,0,0\n'})}),"\n",(0,o.jsx)(n.p,{children:"In this error message, you can see that the Workload Group used 1.7GB of memory, while the Workload Group's limit is 1.69GB. The calculation works as follows: 1.69GB = Physical machine memory (375GB) * mem_limit (value in be.conf, default is 0.9) * 0.5% (Workload Group's configuration). This means that the memory percentage configured in the Workload Group is calculated based on the available memory of the Doris process."}),"\n",(0,o.jsx)(n.h3,{id:"suggestions",children:"Suggestions"}),"\n",(0,o.jsx)(n.p,{children:"As demonstrated in the previous test, the hard limit can control the memory usage of a Workload Group, but it releases memory by killing queries, which can be a very unfriendly experience for users and may cause all queries to fail in extreme cases. Therefore, in a production environment, it is recommended to use memory hard limits in conjunction with query queuing. This approach can limit memory usage while ensuring the success rate of queries."}),"\n",(0,o.jsx)(n.h2,{id:"cpu-hard-limit-test",children:"CPU hard limit Test"}),"\n",(0,o.jsx)(n.p,{children:"The workloads in Doris can generally be divided into three categories:"}),"\n",(0,o.jsx)(n.p,{children:"Core Report Queries: These are typically used by company executives to view reports. The load may not be very high, but the availability requirements are high. These queries can be grouped together and assigned a higher-priority soft limit, ensuring they receive more CPU resources when CPU availability is limited."}),"\n",(0,o.jsx)(n.p,{children:"Ad-hoc Queries: These queries are usually exploratory, with random SQL and unpredictable resource usage. They typically have a lower priority, so they can be managed with a CPU hard limit set to a lower value, preventing them from consuming excessive CPU resources and reducing cluster availability."}),"\n",(0,o.jsx)(n.p,{children:"ETL Queries: These queries have relatively fixed SQL and stable resource usage, though occasionally, a sudden increase in upstream data volume can cause a spike in resource usage. Therefore, they can be managed with a CPU hard limit configuration."}),"\n",(0,o.jsx)(n.p,{children:"Different workloads consume CPU resources differently, and users have varying requirements for response latency. When the BE's CPU is heavily utilized, availability decreases and response latency increases. For example, an ad-hoc analytical query might fully utilize the cluster's CPU, causing increased latency for core reports and impacting the SLA. Therefore, a CPU isolation mechanism is needed to separate different types of workloads, ensuring cluster availability and meeting SLAs."}),"\n",(0,o.jsx)(n.p,{children:"Workload Groups support both soft and hard CPU limits, with a current recommendation to configure Workload Groups with hard limits in production environments. This is because soft limits only come into effect when the CPU is fully utilized, but when the CPU is maxed out, internal components of Doris (e.g., RPC components) and available CPU resources for the operating system are reduced, leading to a significant decline in overall cluster availability. Therefore, in production environments, it's generally necessary to avoid situations where CPU resources are fully utilized, and the same applies to other resources such as memory."}),"\n",(0,o.jsx)(n.h3,{id:"test-env-1",children:"Test env"}),"\n",(0,o.jsx)(n.p,{children:"1FE, 1BE(96 cores)\nTest data is clickbench\uFF0Csql is q29."}),"\n",(0,o.jsx)(n.h3,{id:"test",children:"Test"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"Run query in 3 concurrent, using top command we can see it uses 76 cores."}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.img,{alt:"use workload group cpu",src:r(53982).Z+"",width:"962",height:"146"})}),"\n",(0,o.jsxs)(n.ol,{start:"2",children:["\n",(0,o.jsx)(n.li,{children:"Alter workload group."}),"\n"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:"alter workload group g2 properties('cpu_hard_limit'='10%');\n"})}),"\n",(0,o.jsxs)(n.ol,{start:"3",children:["\n",(0,o.jsx)(n.li,{children:"Enable cpu hard limit\uFF0Cthen all workload group could convert to hard limit."}),"\n"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:'ADMIN SET FRONTEND CONFIG ("enable_cpu_hard_limit" = "true");\n'})}),"\n",(0,o.jsxs)(n.ol,{start:"4",children:["\n",(0,o.jsx)(n.li,{children:"Test again, the BE using 9 ~ 10 cores, about 10%."}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.img,{alt:"use workload group cpu",src:r(665930).Z+"",width:"1138",height:"180"})}),"\n",(0,o.jsx)(n.p,{children:"It should be noted that it's best to use query workloads for testing here, as they are more likely to reflect the intended effects. If you use high-throughput data import instead, it may trigger compaction, causing the observed values to be higher than the configured Workload Group limits. Currently, the compaction workload is not managed by the Workload Group."}),"\n",(0,o.jsxs)(n.ol,{start:"5",children:["\n",(0,o.jsx)(n.li,{children:"Use system table to show cpu usage, it's about 10%;"}),"\n"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:"mysql [information_schema]>select CPU_USAGE_PERCENT from workload_group_resource_usage where WORKLOAD_GROUP_ID=11201;\n+-------------------+\n| CPU_USAGE_PERCENT |\n+-------------------+\n|              9.57 |\n+-------------------+\n1 row in set (0.02 sec)\n"})}),"\n",(0,o.jsx)(n.h3,{id:"note",children:"NOTE"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsx)(n.p,{children:"When configuring in practice, it's best not to have the total CPU allocation of all Groups add up to exactly 100%. This is primarily to ensure availability in low-latency scenarios, as some resources need to be reserved for other components. However, if the scenario is not sensitive to latency and the goal is maximum resource utilization, you can consider configuring the total CPU allocation of all Groups to equal 100%."}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsx)(n.p,{children:"Currently, the time interval for the Frontend (FE) to synchronize Workload Group metadata to the Backend (BE) is 30 seconds, so it may take up to 30 seconds for changes to the Workload Group to take effect."}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"test-limit-local-io",children:"Test limit local IO"}),"\n",(0,o.jsx)(n.p,{children:"In OLAP systems, when performing ETL or large ad-hoc queries, a significant amount of data needs to be read. To speed up data analysis, Doris internally uses multithreading to scan multiple disk files in parallel, which generates a large amount of disk I/O and can negatively impact other queries, such as report analysis."}),"\n",(0,o.jsx)(n.p,{children:"By using Workload Groups, you can group offline ETL data processing and online report queries separately and limit the I/O bandwidth for offline data processing, thereby reducing its impact on online report analysis."}),"\n",(0,o.jsx)(n.h3,{id:"test-1",children:"Test"}),"\n",(0,o.jsx)(n.p,{children:"1FE,1BE(96 cores), test data is clickbench"}),"\n",(0,o.jsx)(n.h3,{id:"not-limit-io",children:"Not limit IO"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"Clear cache."}),"\n"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:"// clear OS cache.\nsync; echo 3 > /proc/sys/vm/drop_caches\n\n// disable BE's cache.\ndisable_storage_page_cache = true\n"})}),"\n",(0,o.jsxs)(n.ol,{start:"2",children:["\n",(0,o.jsx)(n.li,{children:"Run query one by one."}),"\n"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:"set dry_run_query = true;\nselect * from hits.hits;\n"})}),"\n",(0,o.jsxs)(n.ol,{start:"3",children:["\n",(0,o.jsx)(n.li,{children:"Show local IO by system table, is's 3G/s."}),"\n"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:"mysql [information_schema]>select LOCAL_SCAN_BYTES_PER_SECOND / 1024 / 1024 as mb_per_sec from workload_group_resource_usage where WORKLOAD_GROUP_ID=11201;\n+--------------------+\n| mb_per_sec         |\n+--------------------+\n| 1146.6208400726318 |\n+--------------------+\n1 row in set (0.03 sec)\n\nmysql [information_schema]>select LOCAL_SCAN_BYTES_PER_SECOND / 1024 / 1024 as mb_per_sec from workload_group_resource_usage where WORKLOAD_GROUP_ID=11201;\n+--------------------+\n| mb_per_sec         |\n+--------------------+\n| 3496.2762966156006 |\n+--------------------+\n1 row in set (0.04 sec)\n\nmysql [information_schema]>select LOCAL_SCAN_BYTES_PER_SECOND / 1024 / 1024 as mb_per_sec from workload_group_resource_usage where WORKLOAD_GROUP_ID=11201;\n+--------------------+\n| mb_per_sec         |\n+--------------------+\n| 2192.7690029144287 |\n+--------------------+\n1 row in set (0.02 sec)\n"})}),"\n",(0,o.jsx)(n.p,{children:"4.Show IO by pidstat, the first column in picture is process id, the second column is IO(kb/s), it's 2G/s."}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.img,{alt:"use workload group io",src:r(715719).Z+"",width:"814",height:"1080"})}),"\n",(0,o.jsx)(n.h3,{id:"test-io-limit",children:"Test IO limit."}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"Clear cache."}),"\n"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:"// clear os cache\nsync; echo 3 > /proc/sys/vm/drop_caches\n\n// disable BE cache\ndisable_storage_page_cache = true\n"})}),"\n",(0,o.jsxs)(n.ol,{start:"2",children:["\n",(0,o.jsx)(n.li,{children:"Alter workload group."}),"\n"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:"alter workload group g2 properties('read_bytes_per_second'='104857600');\n"})}),"\n",(0,o.jsxs)(n.ol,{start:"3",children:["\n",(0,o.jsx)(n.li,{children:"Show IO by system table, it's about 98M/s."}),"\n"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:"mysql [information_schema]>select LOCAL_SCAN_BYTES_PER_SECOND / 1024 / 1024 as mb_per_sec from workload_group_resource_usage where WORKLOAD_GROUP_ID=11201;\n+--------------------+\n| mb_per_sec         |\n+--------------------+\n| 97.94296646118164  |\n+--------------------+\n1 row in set (0.03 sec)\n\nmysql [information_schema]>select LOCAL_SCAN_BYTES_PER_SECOND / 1024 / 1024 as mb_per_sec from workload_group_resource_usage where WORKLOAD_GROUP_ID=11201;\n+--------------------+\n| mb_per_sec         |\n+--------------------+\n| 98.37584781646729  |\n+--------------------+\n1 row in set (0.04 sec)\n\nmysql [information_schema]>select LOCAL_SCAN_BYTES_PER_SECOND / 1024 / 1024 as mb_per_sec from workload_group_resource_usage where WORKLOAD_GROUP_ID=11201;\n+--------------------+\n| mb_per_sec         |\n+--------------------+\n| 98.06641292572021  |\n+--------------------+\n1 row in set (0.02 sec)\n"})}),"\n",(0,o.jsxs)(n.ol,{start:"4",children:["\n",(0,o.jsx)(n.li,{children:"Show IO by pidstat, the process IO is about 131M/s\u3002"}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.img,{alt:"use workload group io",src:r(579239).Z+"",width:"808",height:"676"})}),"\n",(0,o.jsx)(n.h3,{id:"note-1",children:"NOTE"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"The LOCAL_SCAN_BYTES_PER_SECOND field in the system table represents the aggregated statistics at the process level for the current Workload Group. For example, if 12 file paths are configured, LOCAL_SCAN_BYTES_PER_SECOND represents the maximum I/O value across these 12 file paths. If you want to see the I/O throughput for each file path individually, you can view detailed values in Grafana or through the BE's bvar monitoring."}),"\n",(0,o.jsx)(n.li,{children:"Due to the presence of the operating system's and Doris's Page Cache, the I/O values observed using Linux's I/O monitoring scripts are usually smaller than those seen in the system table."}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"BrokerLoad and S3Load are commonly used methods for importing large volumes of data. Users can first upload data to HDFS or S3 and then use BrokerLoad and S3Load for parallel data imports. To speed up the import process, Doris uses multithreading to pull data from HDFS/S3 in parallel. This can put significant pressure on HDFS/S3, potentially causing instability for other jobs running on HDFS/S3."}),"\n",(0,o.jsx)(n.p,{children:"You can use the remote I/O limitation feature of Workload Groups to limit the bandwidth used during the import process, thereby reducing the impact on other operations."}),"\n",(0,o.jsx)(n.h2,{id:"test-limit-remote-io",children:"Test limit remote IO"}),"\n",(0,o.jsx)(n.h3,{id:"test-env-2",children:"Test env"}),"\n",(0,o.jsx)(n.p,{children:"1FE,1BE(16 cores, 64G), test data is clickbench,Before testing, the dataset needs to be uploaded to S3. To save time, we will upload only 10 million rows of data, and then use the TVF (Table-Valued Function) feature to query the data from S3."}),"\n",(0,o.jsx)(n.p,{children:"Show schema info after upload."}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:'DESC FUNCTION s3 (\n    "URI" = "https://bucketname/1kw.tsv",\n    "s3.access_key"= "ak",\n    "s3.secret_key" = "sk",\n    "format" = "csv",\n    "use_path_style"="true"\n);\n'})}),"\n",(0,o.jsx)(n.h3,{id:"test-not-limit-remote-io",children:"Test not limit remote IO"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"Run query one by one."}),"\n"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:'// just scan, not return value.\nset dry_run_query = true;\n\nSELECT * FROM s3(\n    "URI" = "https://bucketname/1kw.tsv",\n    "s3.access_key"= "ak",\n    "s3.secret_key" = "sk",\n    "format" = "csv",\n    "use_path_style"="true"\n);\n'})}),"\n",(0,o.jsxs)(n.ol,{start:"2",children:["\n",(0,o.jsx)(n.li,{children:"Show remote IO by system table,it's about 837M/s, It should be noted that the actual I/O throughput here is significantly affected by the environment. If the machine hosting the BE has a low bandwidth connection to external storage, the actual throughput may be lower."}),"\n"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:"MySQL [(none)]> select cast(REMOTE_SCAN_BYTES_PER_SECOND/1024/1024 as int) as read_mb from information_schema.workload_group_resource_usage;\n+---------+\n| read_mb |\n+---------+\n|     837 |\n+---------+\n1 row in set (0.104 sec)\n\nMySQL [(none)]> select cast(REMOTE_SCAN_BYTES_PER_SECOND/1024/1024 as int) as read_mb from information_schema.workload_group_resource_usage;\n+---------+\n| read_mb |\n+---------+\n|     867 |\n+---------+\n1 row in set (0.070 sec)\n\nMySQL [(none)]> select cast(REMOTE_SCAN_BYTES_PER_SECOND/1024/1024 as int) as read_mb from information_schema.workload_group_resource_usage;\n+---------+\n| read_mb |\n+---------+\n|     867 |\n+---------+\n1 row in set (0.186 sec)\n"})}),"\n",(0,o.jsxs)(n.ol,{start:"3",children:["\n",(0,o.jsx)(n.li,{children:"Using sar(sar -n DEV 1 3600) to show network bandwidth of the machine, the max value is about 1033M/s.The first column of the output is the number of bytes received per second by a certain network card of the current machine, in KB per second."}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.img,{alt:"use workload group rio",src:r(716653).Z+"",width:"960",height:"436"})}),"\n",(0,o.jsx)(n.h3,{id:"test-limit-remote-io-1",children:"Test limit remote IO"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"Alter workload group."}),"\n"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:"alter workload group normal properties('remote_read_bytes_per_second'='104857600');\n"})}),"\n",(0,o.jsxs)(n.ol,{start:"2",children:["\n",(0,o.jsx)(n.li,{children:"Run query one by one."}),"\n"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:'// just scan not return.\nset dry_run_query = true;\n\n\nSELECT * FROM s3(\n    "URI" = "https://bucketname/1kw.tsv",\n    "s3.access_key"= "ak",\n    "s3.secret_key" = "sk",\n    "format" = "csv",\n    "use_path_style"="true"\n);\n'})}),"\n",(0,o.jsxs)(n.ol,{start:"3",children:["\n",(0,o.jsx)(n.li,{children:"Use the system table to check the current remote read I/O throughput. At this moment, the I/O throughput is around 100M, with some fluctuation. This fluctuation is influenced by the current algorithm design and typically includes a peak, but it does not last long and is considered normal."}),"\n"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:"MySQL [(none)]> select cast(REMOTE_SCAN_BYTES_PER_SECOND/1024/1024 as int) as read_mb from information_schema.workload_group_resource_usage;\n+---------+\n| read_mb |\n+---------+\n|      56 |\n+---------+\n1 row in set (0.010 sec)\n\nMySQL [(none)]> select cast(REMOTE_SCAN_BYTES_PER_SECOND/1024/1024 as int) as read_mb from information_schema.workload_group_resource_usage;\n+---------+\n| read_mb |\n+---------+\n|     131 |\n+---------+\n1 row in set (0.009 sec)\n\nMySQL [(none)]> select cast(REMOTE_SCAN_BYTES_PER_SECOND/1024/1024 as int) as read_mb from information_schema.workload_group_resource_usage;\n+---------+\n| read_mb |\n+---------+\n|     111 |\n+---------+\n1 row in set (0.009 sec)\n"})}),"\n",(0,o.jsxs)(n.ol,{start:"4",children:["\n",(0,o.jsx)(n.li,{children:"Using sar(sar -n DEV 1 3600) to show network bandwidth, the max IO is about 207M, This indicates that remote limit IO works. However, since the sar command shows machine-level traffic, the values may be higher than those reported by Doris."}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.img,{alt:"use workload group rio",src:r(674967).Z+"",width:"864",height:"472"})})]})}function u(e={}){let{wrapper:n}={...(0,t.a)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},53982:function(e,n,r){r.d(n,{Z:function(){return s}});let s=r.p+"assets/images/use_wg_cpu_1-6211d7170720ad508590d94cf72724e3.png"},665930:function(e,n,r){r.d(n,{Z:function(){return s}});let s=r.p+"assets/images/use_wg_cpu_2-03419ba3e02b1980179a1af557d41d1c.png"},715719:function(e,n,r){r.d(n,{Z:function(){return s}});let s=r.p+"assets/images/use_wg_io_1-15b5d3e97f7ccf7f914eef0e6bf55cd4.png"},579239:function(e,n,r){r.d(n,{Z:function(){return s}});let s=r.p+"assets/images/use_wg_io_2-2202fea206c596a3879c6660ada71037.png"},716653:function(e,n,r){r.d(n,{Z:function(){return s}});let s=r.p+"assets/images/use_wg_rio_1-e3ae321d5e709bb0ba21b9572d2bce1f.png"},674967:function(e,n,r){r.d(n,{Z:function(){return s}});let s=r.p+"assets/images/use_wg_rio_2-7511c9afedf43b931909b18e529f00ea.png"},250065:function(e,n,r){r.d(n,{Z:function(){return a},a:function(){return i}});var s=r(667294);let o={},t=s.createContext(o);function i(e){let n=s.useContext(t);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:i(e.components),s.createElement(t.Provider,{value:n},e.children)}}}]);